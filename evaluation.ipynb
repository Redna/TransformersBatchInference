{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline\n",
    "\n",
    "import re\n",
    "from langchain.chains import LLMChain\n",
    "import torch\n",
    "\n",
    "\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.pydantic_v1 import Extra, root_validator\n",
    "from langchain.utils import get_from_dict_or_env\n",
    "\n",
    "VALID_TASKS = (\"text2text-generation\", \"text-generation\", \"summarization\")\n",
    "\n",
    "\n",
    "class TransformersBatchInference(LLM):\n",
    "\n",
    "    endpoint_url: str = \"\"\n",
    "    \"\"\"Endpoint URL to use.\"\"\"\n",
    "\n",
    "    model_kwargs: Optional[dict] = None\n",
    "    \"\"\"Key word arguments to pass to the model.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        _model_kwargs = self.model_kwargs or {}\n",
    "        return {\n",
    "            **{\"endpoint_url\": self.endpoint_url},\n",
    "            **{\"model_kwargs\": _model_kwargs},\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"huggingface_endpoint\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call out to HuggingFace Hub's inference endpoint.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                response = hf(\"Tell me a joke.\")\n",
    "        \"\"\"\n",
    "        _model_kwargs = self.model_kwargs or {}\n",
    "\n",
    "        # payload samples\n",
    "        params = {**_model_kwargs, **kwargs}\n",
    "        parameter_payload = {\"inputs\": prompt, \"parameters\": params}\n",
    "\n",
    "        # HTTP headers for authorization\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.endpoint_url, headers=headers, json=parameter_payload\n",
    "            )\n",
    "        except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "            raise ValueError(f\"Error raised by inference endpoint: {e}\")\n",
    "       \n",
    "        generated_text = response.json()\n",
    "        if \"error\" in generated_text:\n",
    "            raise ValueError(\n",
    "                f\"Error raised by inference API: {generated_text['error']}\"\n",
    "            )\n",
    "        \n",
    "        text = generated_text[0][\"generated_text\"]\n",
    "        if stop is not None:\n",
    "            # This is a bit hacky, but I can't figure out a better way to enforce\n",
    "            # stop tokens when making calls to huggingface_hub.\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "        return text\n",
    "\n",
    "\n",
    "llm = TransformersBatchInference(endpoint_url=\"http://localhost:30091/v1/generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"What is concious thinking?\",\n",
    "     \"How do you know if you are concious?\",\n",
    "     \"What is reality?\", \n",
    "     \"When will the world end?\",\n",
    "     \"Why is the sky blue?\",\n",
    "     \"When is the next world war?\",\n",
    "     \"What is a black hole?\",\n",
    "     \"What is a quark?\",\n",
    "     \"What is a photon?\",\n",
    "     \"What is a gluon?\"\n",
    "     \"Is there a god?\",\n",
    "     \"What is the meaning of life?\",\n",
    "     \"What is the meaning of death?\",\n",
    "     \"What is the meaning of conciousness?\",\n",
    "     \"What is the meaning of reality?\",\n",
    "     \"What is the meaning of existence?\",\n",
    "     \"What is the meaning of the universe?\",\n",
    "     \"What is the meaning of the multiverse?\",\n",
    "     \"When does the universe end?\",\n",
    "     \"What is the universe expanding into?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [02:25<00:00,  7.65s/it]\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "\n",
    "for i in tqdm(examples, total=len(examples)):\n",
    "    responses.append(await llm.agenerate([\"What is the purpose of life?\"], \n",
    "        max_length = 300, \n",
    "        top_p = 0.95, \n",
    "        top_k = 50, \n",
    "        do_sample = True, \n",
    "        num_return_sequences = 1, \n",
    "        temperature = 0.4, \n",
    "        repetition_penalty = 1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='What is the purpose of life?\\n- How can I be a better person and make a positive impact on others?\\n- Is there any way to transcend my limitations or overcome suffering?\\n\\nThese questions have been asked by people throughout history, across cultures and religions. While different traditions may offer unique answers, they all share a common goal: to help us understand our place in the world, find meaning and purpose, and ultimately achieve some form of enlightenment or liberation from suffering.')]] llm_output=None run=[RunInfo(run_id=UUID('85a23698-4747-474c-9e04-98e42aa42f98'))]\n"
     ]
    }
   ],
   "source": [
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 196769.82it/s]\n"
     ]
    }
   ],
   "source": [
    "calls = []\n",
    "\n",
    "for i in tqdm(examples, total=len(examples)):\n",
    "    calls.append(llm.agenerate([\"What is the purpose of life?\"], \n",
    "        max_length = 300, \n",
    "        top_p = 0.95, \n",
    "        top_k = 50, \n",
    "        do_sample = True, \n",
    "        num_return_sequences = 1, \n",
    "        temperature = 0.4, \n",
    "        repetition_penalty = 1.2))\n",
    "\n",
    "reponses_batch = await asyncio.gather(*calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
